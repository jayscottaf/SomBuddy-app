You are helping build the backend logic for the SomBuddy AI assistant app. The assistant uses GPT-4o to analyze user-uploaded images and provide wine pairing suggestions.

ğŸ“¸ Users may upload:
- Wine menus (e.g., restaurant lists)
- Meal photos (e.g., steak, pasta)
- Wine bottle labels (e.g., to understand what wine theyâ€™re drinking)

â—The current issue:
The app always sends a hardcoded prompt (e.g., assuming the image is a wine menu), which causes GPT-4o to hallucinate if the image is actually a meal or wine bottle.

âœ… Goal:
Create a smart system that detects the image type before crafting the GPT prompt, so responses are accurate and contextual.

ğŸ”§ Your Tasks:
1. Add a helper function called `detectImageType(fileId)` using GPT-4o Vision. It should:
   - Accept a `fileId` from an uploaded image
   - Prompt GPT to classify the image as one of:
     - "wine menu"
     - "meal photo"
     - "wine bottle"
     - "other"
   - Return that tag as a string
   - Fallback to `"other"` if classification fails

2. Modify the message-sending logic so that:
   - When an image is uploaded, it is first uploaded to OpenAI and `fileId` is retrieved
   - `detectImageType(fileId)` is run
   - Based on the result, choose the correct GPT-4o prompt:
     - If `"wine menu"` â†’ extract menu text, then pair based only on wines listed
     - If `"meal photo"` â†’ describe the dish visually and suggest matching wines
     - If `"wine bottle"` â†’ identify the wine and suggest what foods go best with it
     - If `"other"` â†’ respond with a friendly fallback like: â€œThis looks interesting! Let me know if it's a meal, menu, or wine bottle and Iâ€™ll help pair it properly.â€

3. If needed, log the detected image type and the generated prompt for debugging.

4. Make the entire process invisible to the user â€” they should never need to select an image type manually.

ğŸ“‹ Please answer these clarifying questions before proceeding:
- Where exactly is the message-sending logic located â€” in a backend API route (like `/api/assistant/message`) or directly in the frontend (e.g., `ChatPage.tsx`)?
- Is the image upload already sending the image to OpenAI and retrieving a `file_id`, or does it use base64 right now?
- Would you like to store the image type (e.g., in session, thread metadata, or console log) for future reference or debugging?
- If image type detection returns `"other"`, should the assistant respond automatically, or prompt the user with buttons (e.g., â€œIs this a meal, menu, or wine bottle?â€)?

ğŸ¯ Objective:
Refactor the system to route image-based requests through GPT-4oâ€™s vision with the right intent and prompt, without any user intervention. Keep it modular and production-ready.